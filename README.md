# **Sign Language Translator**

## **ğŸ“Œ Overview**

The Sign Language Translator is an AI-powered application that helps translate American Sign Language (ASL) alphabet gestures in real-time. It uses a pre-trained model from Hugging Face to recognize ASL letters from images, camera input, and live video feeds.

## **ğŸš€ Features**

ğŸ“¸ Image Load: Upload an image to detect ASL alphabet gestures.

ğŸ“· Take Picture: Capture a picture using your camera and classify the gesture.

ğŸ¥ Live ASL Mode: Enable your webcam for real-time ASL alphabet recognition.

ğŸ“– About Us: Information about the project and its purpose.

ğŸ“ Contact Us: Dummy phone number, LinkedIn, Facebook, Instagram links, and email.

ğŸ’¬ Feedback: A section to collect user feedback.

## **ğŸ— Technologies Used**

Python ğŸ

Streamlit ğŸ¨ (for the interactive UI)

Hugging Face Transformers ğŸ¤— (for ASL classification)

OpenCV ğŸ“· (for handling live video input)

Torch ğŸ”¥ (for deep learning model inference)

## **ğŸ›  Installation & Setup**

***Clone the repository:***
git clone https://github.com/your-username/sign-language-translator.git
cd sign-language-translator

***Create a virtual environment*** (optional but recommended):
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate

***Install dependencies:****
pip install -r requirements.txt

***Run the application:****
streamlit run app.py

## **ğŸ“„ Project Structure**

ğŸ“‚ sign-language-translator
â”‚â”€â”€ ğŸ“œ app.py               # Main application script
â”‚â”€â”€ ğŸ“œ requirements.txt      # List of dependencies
â”‚â”€â”€ ğŸ“œ README.md             # Project documentation (this file)

## **ğŸ”¥ How It Works**

Load an image, take a picture, or enable live mode.

The AI model processes the input and classifies the ASL gesture.

The recognized letter is displayed to the user.
